"""
Evaluation models for testing and improving the dental sequence generator
"""
from app import db
from datetime import datetime
from sqlalchemy.dialects.postgresql import JSON
from sqlalchemy import Index


class EvaluationTestCase(db.Model):
    """Test cases for evaluating sequence generation quality"""
    __tablename__ = 'evaluation_test_cases'
    
    id = db.Column(db.Integer, primary_key=True)
    consultation_text = db.Column(db.Text, nullable=False)
    patient_context = db.Column(JSON)  # Age, medical history, etc.
    gold_standard_sequence = db.Column(JSON)  # Dentist-provided ideal sequence
    difficulty_level = db.Column(db.String(20))  # easy, medium, hard, expert
    categories = db.Column(JSON)  # ['endodontics', 'prosthetics', etc.]
    notes = db.Column(db.Text)  # Additional context or special considerations
    
    # Tracking
    created_by_id = db.Column(db.Integer, db.ForeignKey('users.id'))
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    updated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    is_active = db.Column(db.Boolean, default=True)
    
    # Relationships
    created_by = db.relationship('User', backref='created_test_cases')
    generated_sequences = db.relationship('GeneratedSequence', back_populates='test_case', 
                                        cascade='all, delete-orphan')
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_test_case_difficulty', 'difficulty_level'),
        Index('idx_test_case_active', 'is_active'),
    )
    
    def to_dict(self):
        return {
            'id': self.id,
            'consultation_text': self.consultation_text,
            'patient_context': self.patient_context or {},
            'gold_standard_sequence': self.gold_standard_sequence or [],
            'difficulty_level': self.difficulty_level,
            'categories': self.categories or [],
            'notes': self.notes,
            'created_by': self.created_by.username if self.created_by else None,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None,
            'evaluation_count': len(self.generated_sequences),
            'average_score': self.get_average_score()
        }
    
    def get_average_score(self):
        """Calculate average score across all evaluations"""
        evaluated_sequences = [seq for seq in self.generated_sequences 
                             if seq.manual_evaluations]
        if not evaluated_sequences:
            return None
        
        total_score = sum(eval.overall_score for seq in evaluated_sequences 
                         for eval in seq.manual_evaluations if eval.overall_score)
        eval_count = sum(1 for seq in evaluated_sequences 
                        for eval in seq.manual_evaluations if eval.overall_score)
        
        return round(total_score / eval_count, 2) if eval_count > 0 else None


class GeneratedSequence(db.Model):
    """Sequences generated by the AI for evaluation"""
    __tablename__ = 'generated_sequences'
    
    id = db.Column(db.Integer, primary_key=True)
    test_case_id = db.Column(db.Integer, db.ForeignKey('evaluation_test_cases.id'), nullable=False)
    
    # Generation details
    model_version = db.Column(db.String(50))  # Track which model version
    generated_sequence = db.Column(JSON, nullable=False)
    generation_params = db.Column(JSON)  # Temperature, settings, etc.
    rag_references = db.Column(JSON)  # Which references were used
    generation_time_ms = db.Column(db.Integer)  # Performance tracking
    
    # Timestamps
    generated_at = db.Column(db.DateTime, default=datetime.utcnow)
    
    # Relationships
    test_case = db.relationship('EvaluationTestCase', back_populates='generated_sequences')
    manual_evaluations = db.relationship('ManualEvaluation', back_populates='generated_sequence',
                                       cascade='all, delete-orphan')
    automatic_evaluation = db.relationship('AutomaticEvaluation', back_populates='generated_sequence',
                                         uselist=False, cascade='all, delete-orphan')
    
    # Indexes
    __table_args__ = (
        Index('idx_generated_test_case', 'test_case_id'),
        Index('idx_generated_timestamp', 'generated_at'),
    )
    
    @property
    def evaluation_status(self):
        """Return evaluation status: pending, partial, complete"""
        if not self.manual_evaluations and not self.automatic_evaluation:
            return 'pending'
        elif self.manual_evaluations and self.automatic_evaluation:
            return 'complete'
        else:
            return 'partial'
    
    @property
    def average_manual_score(self):
        """Calculate average manual evaluation score"""
        if not self.manual_evaluations:
            return None
        
        scores = [e.overall_score for e in self.manual_evaluations if e.overall_score]
        return round(sum(scores) / len(scores), 2) if scores else None
    
    def to_dict(self):
        return {
            'id': self.id,
            'test_case_id': self.test_case_id,
            'model_version': self.model_version,
            'generated_sequence': self.generated_sequence,
            'generation_params': self.generation_params,
            'rag_references': self.rag_references,
            'generation_time_ms': self.generation_time_ms,
            'generated_at': self.generated_at.isoformat() if self.generated_at else None,
            'evaluation_status': self.evaluation_status,
            'average_manual_score': self.average_manual_score,
            'automatic_score': self.automatic_evaluation.overall_score if self.automatic_evaluation else None,
            'manual_evaluation_count': len(self.manual_evaluations)
        }


class ManualEvaluation(db.Model):
    """Manual evaluations by dental professionals"""
    __tablename__ = 'manual_evaluations'
    
    id = db.Column(db.Integer, primary_key=True)
    generated_sequence_id = db.Column(db.Integer, db.ForeignKey('generated_sequences.id'), nullable=False)
    evaluator_id = db.Column(db.Integer, db.ForeignKey('users.id'), nullable=False)
    
    # Scoring rubric (0-10 scale)
    clinical_accuracy_score = db.Column(db.Float)
    sequencing_logic_score = db.Column(db.Float)
    cost_appropriateness_score = db.Column(db.Float)
    safety_score = db.Column(db.Float)
    completeness_score = db.Column(db.Float)
    overall_score = db.Column(db.Float, nullable=False)
    
    # Detailed feedback
    strengths = db.Column(db.Text)
    weaknesses = db.Column(db.Text)
    suggestions = db.Column(db.Text)
    
    # Timestamps
    evaluated_at = db.Column(db.DateTime, default=datetime.utcnow)
    
    # Relationships
    generated_sequence = db.relationship('GeneratedSequence', back_populates='manual_evaluations')
    evaluator = db.relationship('User', backref='sequence_evaluations')
    
    # Indexes
    __table_args__ = (
        Index('idx_manual_eval_sequence', 'generated_sequence_id'),
        Index('idx_manual_eval_evaluator', 'evaluator_id'),
        Index('idx_manual_eval_timestamp', 'evaluated_at'),
    )
    
    def to_dict(self):
        return {
            'id': self.id,
            'generated_sequence_id': self.generated_sequence_id,
            'evaluator': self.evaluator.username if self.evaluator else None,
            'scores': {
                'clinical_accuracy': self.clinical_accuracy_score,
                'sequencing_logic': self.sequencing_logic_score,
                'cost_appropriateness': self.cost_appropriateness_score,
                'safety': self.safety_score,
                'completeness': self.completeness_score,
                'overall': self.overall_score
            },
            'feedback': {
                'strengths': self.strengths,
                'weaknesses': self.weaknesses,
                'suggestions': self.suggestions
            },
            'evaluated_at': self.evaluated_at.isoformat() if self.evaluated_at else None
        }


class AutomaticEvaluation(db.Model):
    """Automatic LLM-based evaluations"""
    __tablename__ = 'automatic_evaluations'
    
    id = db.Column(db.Integer, primary_key=True)
    generated_sequence_id = db.Column(db.Integer, db.ForeignKey('generated_sequences.id'), 
                                    nullable=False, unique=True)
    
    # Automated scoring metrics
    semantic_similarity_score = db.Column(db.Float)  # vs gold standard
    completeness_score = db.Column(db.Float)  # All necessary treatments included
    logic_consistency_score = db.Column(db.Float)  # Sequence makes clinical sense
    cost_deviation_percentage = db.Column(db.Float)  # % deviation from expected
    safety_check_passed = db.Column(db.Boolean)
    overall_score = db.Column(db.Float)
    
    # LLM analysis
    llm_feedback = db.Column(JSON)  # Structured feedback from evaluator LLM
    detected_issues = db.Column(JSON)  # List of potential problems
    comparison_details = db.Column(JSON)  # Detailed comparison with gold standard
    
    # Metadata
    evaluation_model = db.Column(db.String(50))  # Which LLM was used
    evaluated_at = db.Column(db.DateTime, default=datetime.utcnow)
    
    # Relationships
    generated_sequence = db.relationship('GeneratedSequence', back_populates='automatic_evaluation')
    
    def to_dict(self):
        return {
            'id': self.id,
            'generated_sequence_id': self.generated_sequence_id,
            'scores': {
                'semantic_similarity': self.semantic_similarity_score,
                'completeness': self.completeness_score,
                'logic_consistency': self.logic_consistency_score,
                'cost_deviation_percentage': self.cost_deviation_percentage,
                'safety_check_passed': self.safety_check_passed,
                'overall': self.overall_score
            },
            'llm_feedback': self.llm_feedback or {},
            'detected_issues': self.detected_issues or [],
            'comparison_details': self.comparison_details or {},
            'evaluation_model': self.evaluation_model,
            'evaluated_at': self.evaluated_at.isoformat() if self.evaluated_at else None
        }


class EvaluationMetrics(db.Model):
    """Aggregated metrics for tracking system performance over time"""
    __tablename__ = 'evaluation_metrics'
    
    id = db.Column(db.Integer, primary_key=True)
    
    # Time period
    metric_date = db.Column(db.Date, nullable=False)
    model_version = db.Column(db.String(50))
    
    # Aggregate scores
    avg_manual_score = db.Column(db.Float)
    avg_automatic_score = db.Column(db.Float)
    avg_clinical_accuracy = db.Column(db.Float)
    avg_cost_appropriateness = db.Column(db.Float)
    avg_safety_score = db.Column(db.Float)
    
    # Volume metrics
    sequences_generated = db.Column(db.Integer, default=0)
    sequences_evaluated = db.Column(db.Integer, default=0)
    unique_evaluators = db.Column(db.Integer, default=0)
    
    # Performance metrics
    avg_generation_time_ms = db.Column(db.Float)
    error_rate = db.Column(db.Float)  # % of sequences with major issues
    
    # Timestamps
    calculated_at = db.Column(db.DateTime, default=datetime.utcnow)
    
    # Indexes
    __table_args__ = (
        Index('idx_metrics_date_version', 'metric_date', 'model_version'),
    )